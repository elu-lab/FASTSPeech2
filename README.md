# FASTSPeech2
 TTS(= Text-To-Speech) Model for researching. This Repository is mainly based on [ming024/FastSpeech2](https://github.com/ming024/FastSpeech2) and modified or added some codes for our team's dataset. We use [MLS(=Multilingual LibriSpeech)](https://www.openslr.org/94/) dataset for training. 


## Languages
 We trained FastSpeech2 Model following languages with introducing each language's phonsets we embedded and trained. We used [`Montreal-Forced Alignment`](https://montreal-forced-aligner.readthedocs.io/en/latest/user_guide/workflows/alignment.html) tool to obtain the alignments between the utterances and the phoneme sequences as described in the [paper](https://arxiv.org/pdf/2006.04558.pdf). As you can see, we embedded [`IPA Phoneset`](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet). 
<details>
<summary>German</summary>
<div>
 - <a href="https://mfa-models.readthedocs.io/en/latest/dictionary/German/German%20MFA%20dictionary%20v2_0_0a.html">MFA GERMAN IPA-Phoneset</a> : <code style="white-space:nowrap;">a aj aw a틣 b c c퉗 d e틣 f h i틣 j k k퉗 l l퍌 m m퍌 n n퍌 o틣 p pf p퉗 s t ts t툮 t퉗 u틣 v x y틣 z 칞 칮틣 콂 콉 톓 톖 톖툺 톛 톝  토 톩  투 툮 툵 툺</code>
</div>
</details>

- German
  - [IPA-Phoneset](https://mfa-models.readthedocs.io/en/latest/dictionary/German/German%20MFA%20dictionary%20v2_0_0a.html): `a aj aw a틣 b c c퉗 d e틣 f h i틣 j k k퉗 l l퍌 m m퍌 n n퍌 o틣 p pf p퉗 s t ts t툮 t퉗 u틣 v x y틣 z 칞 칮틣 콂 콉 톓 톖 톖툺 톛 톝  토 톩  투 툮 툵 툺`
- English
  - [IPA-Phoneset](https://mfa-models.readthedocs.io/en/latest/dictionary/English/English%20MFA%20dictionary%20v2_2_1.html): `a aj aw a틣 b b c c퉗 c퉝 d d d d퍍 e ej f f f퉝 h i i틣 j k kp k퉗 k퉝 l m m m퍌 n n퍌 o ow p p퉗 p p퉝 s t t툮 t퉗 t t퉝 t퍍 u u틣 v v v퉝 w z 칝 칞 칧 콂 톓 톔 톔틣  뉆 톖 톖j 톛 톛w 톜 톝 톝틣 톞 톞틣 톟  퉝 토 토b 토퉝 톩 톪 톪퍌  톷  쮉 쮋 툮 툴 툴틣 툵 툹  툾 풪`


## wandb [![wandb](https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-gradient.svg)](https://wandb.ai/wako/FastSpeech2_german)
 If you wanna see the training status, you can check here. You can check theses things above [`wandb link`](https://wandb.ai/wako/FastSpeech2_german):
- Listen to the Samples(= Label Speech & predicted Speech)
- Training / Eval's Mel-Spectrogram


## Preprocess
 This `preprocess.py` can give you the alignments described above. You can get type of alignment data as `TextGrid` after run `preprocess.py`.
```
python preprocess.py config/LibriTTS/preprocess.yaml 
```

## Train
 First, you should log-in wandb with your token key in CLI. 
```
wandb login --relogin '##### Token Key #######'
```

 Next, you can set your training environment with following commands. 
```
accelerate config
```

 With this command, you can start training. 
```
accelerate launch train.py --n_epochs 990 --save_epochs 50 --synthesis_logging_epochs 30 --try_name T4_MoRrgetda
```

Also, you can train your TTS model with this command.
```
CUDA_VISIBLE_DEVICES=0,3 accelerate launch train.py --n_epochs 990 --save_epochs 50 --synthesis_logging_epochs 30 --try_name T4_MoRrgetda
```

 Inference


 Synthesize


## References
- [FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://arxiv.org/abs/2006.04558)
- [ming024/FastSpeech2](https://github.com/ming024/FastSpeech2)
- [pytorch_hub/nvidia/HIFI-GAN](https://pytorch.org/hub/nvidia_deeplearningexamples_hifigan/)
- [游뱅 Accelerate](https://huggingface.co/docs/accelerate/package_reference/accelerator)
- [游뱅 Accelerate(Github)](https://github.com/huggingface/accelerate) 
- [游뱅 examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py)
